# 強化学習の基礎

## マルコフ決定過程(MDP)と部分観測マルコフ決定過程(POMDP)の違い

- MDP
  - 状態が完全に観測できる
- POMDP
  - 状態は一部しか観測できない
  - 状態が直接わからない（潜在変数がある）

## いろんな変数の定義

### 報酬$r_{t+1}$

ある状態$s_t$で行動$a_t$をとったときに得られる報酬・評価値

### 収益$R_t$

報酬の系列$\{r_{t+1}, r_{t+2}, \cdots\}$に対する評価値
一般的には割引報酬和を用いる

$$
R_t = \gamma^0 r_{t+1} + \gamma^1 r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
$$

> [!TIP]
> 割引報酬和(discounted reward sum)：未来の報酬を割引して足し合わせたもの
> 未来の報酬は直近の報酬よりも価値が低いと考えるため、即時的な報酬を重視させるように$\gamma$で割引をかける
